{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"train-balloon.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"FK8Vv-EJBG8s"},"source":["## Training deteksi Balon"]},{"cell_type":"code","metadata":{"id":"5NpuOj8tCN-P"},"source":["import warnings\n","warnings.filterwarnings('ignore')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xUh-tZas_h4e","executionInfo":{"status":"ok","timestamp":1620744697004,"user_tz":-420,"elapsed":71077,"user":{"displayName":"TOMY WIDJAJA","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhVOZPwBedl5N0DM7EP-pKEOGShvrMEj1aEaS-M0Q=s64","userId":"16316982223734628645"}},"outputId":"465831d2-2216-4523-9569-e6b9ab6a9f42"},"source":["!pip install tensorflow-gpu==2.0.0\n","# %tensorflow_version 2.0.0"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting tensorflow-gpu==2.0.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a1/eb/bc0784af18f612838f90419cf4805c37c20ddb957f5ffe0c42144562dcfa/tensorflow_gpu-2.0.0-cp37-cp37m-manylinux2010_x86_64.whl (380.8MB)\n","\u001b[K     |████████████████████████████████| 380.8MB 43kB/s \n","\u001b[?25hRequirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0.0) (1.12.1)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0.0) (0.36.2)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0.0) (1.15.0)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0.0) (3.3.0)\n","Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0.0) (3.12.4)\n","Collecting keras-applications>=1.0.8\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n","\u001b[K     |████████████████████████████████| 51kB 6.5MB/s \n","\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0.0) (0.8.1)\n","Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0.0) (1.19.5)\n","Collecting tensorflow-estimator<2.1.0,>=2.0.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fc/08/8b927337b7019c374719145d1dceba21a8bb909b93b1ad6f8fb7d22c1ca1/tensorflow_estimator-2.0.1-py2.py3-none-any.whl (449kB)\n","\u001b[K     |████████████████████████████████| 450kB 28.0MB/s \n","\u001b[?25hCollecting tensorboard<2.1.0,>=2.0.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/54/99b9d5d52d5cb732f099baaaf7740403e83fe6b0cedde940fabd2b13d75a/tensorboard-2.0.2-py3-none-any.whl (3.8MB)\n","\u001b[K     |████████████████████████████████| 3.8MB 23.3MB/s \n","\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0.0) (1.1.0)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0.0) (1.1.2)\n","Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0.0) (1.32.0)\n","Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0.0) (0.2.0)\n","Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0.0) (0.12.0)\n","Collecting gast==0.2.2\n","  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.6.1->tensorflow-gpu==2.0.0) (56.1.0)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow-gpu==2.0.0) (2.10.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (3.3.4)\n","Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (1.28.1)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (0.4.4)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (1.0.1)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (2.23.0)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (3.10.1)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (0.2.8)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (4.2.1)\n","Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (4.7.2)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (1.3.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (2020.12.5)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (3.7.4.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (3.4.1)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (0.4.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (3.1.0)\n","Building wheels for collected packages: gast\n","  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gast: filename=gast-0.2.2-cp37-none-any.whl size=7540 sha256=790af2d1c6113fc3c30cf8e118c467c5b6ba66245e46466bf01f5413210e5c61\n","  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n","Successfully built gast\n","\u001b[31mERROR: tensorflow 2.4.1 has requirement gast==0.3.3, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n","\u001b[31mERROR: tensorflow 2.4.1 has requirement tensorboard~=2.4, but you'll have tensorboard 2.0.2 which is incompatible.\u001b[0m\n","\u001b[31mERROR: tensorflow 2.4.1 has requirement tensorflow-estimator<2.5.0,>=2.4.0, but you'll have tensorflow-estimator 2.0.1 which is incompatible.\u001b[0m\n","\u001b[31mERROR: tensorflow-probability 0.12.1 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n","Installing collected packages: keras-applications, tensorflow-estimator, tensorboard, gast, tensorflow-gpu\n","  Found existing installation: tensorflow-estimator 2.4.0\n","    Uninstalling tensorflow-estimator-2.4.0:\n","      Successfully uninstalled tensorflow-estimator-2.4.0\n","  Found existing installation: tensorboard 2.4.1\n","    Uninstalling tensorboard-2.4.1:\n","      Successfully uninstalled tensorboard-2.4.1\n","  Found existing installation: gast 0.3.3\n","    Uninstalling gast-0.3.3:\n","      Successfully uninstalled gast-0.3.3\n","Successfully installed gast-0.2.2 keras-applications-1.0.8 tensorboard-2.0.2 tensorflow-estimator-2.0.1 tensorflow-gpu-2.0.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"cyHTqhnfB83s","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620744702946,"user_tz":-420,"elapsed":77012,"user":{"displayName":"TOMY WIDJAJA","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhVOZPwBedl5N0DM7EP-pKEOGShvrMEj1aEaS-M0Q=s64","userId":"16316982223734628645"}},"outputId":"c800840b-64ce-40ea-b426-346f62d66eec"},"source":["!pip install keras==2.3.1"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting keras==2.3.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/fd/6bfe87920d7f4fd475acd28500a42482b6b84479832bdc0fe9e589a60ceb/Keras-2.3.1-py2.py3-none-any.whl (377kB)\n","\u001b[K     |████████████████████████████████| 378kB 4.3MB/s \n","\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from keras==2.3.1) (1.1.2)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras==2.3.1) (3.13)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras==2.3.1) (2.10.0)\n","Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.7/dist-packages (from keras==2.3.1) (1.0.8)\n","Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras==2.3.1) (1.19.5)\n","Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras==2.3.1) (1.4.1)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from keras==2.3.1) (1.15.0)\n","Installing collected packages: keras\n","  Found existing installation: Keras 2.4.3\n","    Uninstalling Keras-2.4.3:\n","      Successfully uninstalled Keras-2.4.3\n","Successfully installed keras-2.3.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"pnOBGYtRCDSo"},"source":["import os\n","import sys\n","import json\n","import datetime\n","import numpy as np\n","import skimage.draw"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OovART_SOA75","executionInfo":{"status":"ok","timestamp":1620745016501,"user_tz":-420,"elapsed":390557,"user":{"displayName":"TOMY WIDJAJA","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhVOZPwBedl5N0DM7EP-pKEOGShvrMEj1aEaS-M0Q=s64","userId":"16316982223734628645"}},"outputId":"a1a12f47-58ef-4eb1-f07e-cfac0568ea3f"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"DoH_jSWSCZcz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620745019014,"user_tz":-420,"elapsed":2504,"user":{"displayName":"TOMY WIDJAJA","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhVOZPwBedl5N0DM7EP-pKEOGShvrMEj1aEaS-M0Q=s64","userId":"16316982223734628645"}},"outputId":"c3253198-71ee-4aa1-88d8-6f3703e391bc"},"source":["# Root directory dari project\n","ROOT_DIR = os.path.abspath(\"/content/drive/MyDrive/Deep Learning/Tugas Kelompok DL/Tugas Implementasi Mask RCNN/Colab/Mask_RCNN-master\")\n","print(ROOT_DIR)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/MyDrive/Deep Learning/Tugas Kelompok DL/Tugas Implementasi Mask RCNN/Colab/Mask_RCNN-master\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lF7fdRfmDScT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620745030955,"user_tz":-420,"elapsed":14438,"user":{"displayName":"TOMY WIDJAJA","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhVOZPwBedl5N0DM7EP-pKEOGShvrMEj1aEaS-M0Q=s64","userId":"16316982223734628645"}},"outputId":"87734751-3d0b-446b-e170-20f081c776b8"},"source":["# Import Mask RCNN\n","sys.path.append(ROOT_DIR)  # To find local version of the library\n","from mrcnn.config import Config\n","from mrcnn import model as modellib, utils\n","\n","# Path to trained weights file\n","COCO_WEIGHTS_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.h5\")\n","\n","# Directory to save logs and model checkpoints, if not provided\n","# through the command line argument --logs\n","DEFAULT_LOGS_DIR = os.path.join(ROOT_DIR, \"logs\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"WmedOotqDblg"},"source":["class BalloonConfig(Config):\n","    \"\"\"Configuration for training on the balloon dataset.\n","    Derives from the base Config class and overrides some values.\n","    \"\"\"\n","    # Give the configuration a recognizable name\n","    NAME = \"balloon\"\n","\n","    # We use a GPU with 12GB memory, which can fit two images.\n","    # Adjust down if you use a smaller GPU.\n","    IMAGES_PER_GPU = 2\n","\n","    # Number of classes (including background)\n","    NUM_CLASSES = 1 + 1  # Background + balloon\n","\n","    # Number of training steps per epoch\n","    STEPS_PER_EPOCH = 100\n","\n","    # Skip detections with < 90% confidence\n","    DETECTION_MIN_CONFIDENCE = 0.9"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TaDw2PUmDkox"},"source":["class BalloonDataset(utils.Dataset):\n","\n","    def load_balloon(self, dataset_dir, subset):\n","        \"\"\"Load a subset of the Balloon dataset.\n","        dataset_dir: Root directory of the dataset.\n","        subset: Subset to load: train or val\n","        \"\"\"\n","        # Add classes. We have only one class to add.\n","        self.add_class(\"balloon\", 1, \"balloon\")\n","\n","        # Train or validation dataset?\n","        assert subset in [\"train\", \"val\"]\n","        dataset_dir = os.path.join(dataset_dir, subset)\n","\n","        # Load annotations\n","        # VGG Image Annotator (up to version 1.6) saves each image in the form:\n","        # { 'filename': '28503151_5b5b7ec140_b.jpg',\n","        #   'regions': {\n","        #       '0': {\n","        #           'region_attributes': {},\n","        #           'shape_attributes': {\n","        #               'all_points_x': [...],\n","        #               'all_points_y': [...],\n","        #               'name': 'polygon'}},\n","        #       ... more regions ...\n","        #   },\n","        #   'size': 100202\n","        # }\n","        # We mostly care about the x and y coordinates of each region\n","        # Note: In VIA 2.0, regions was changed from a dict to a list.\n","        annotations = json.load(open(os.path.join(dataset_dir, \"via_region_data.json\")))\n","        annotations = list(annotations.values())  # don't need the dict keys\n","\n","        # The VIA tool saves images in the JSON even if they don't have any\n","        # annotations. Skip unannotated images.\n","        annotations = [a for a in annotations if a['regions']]\n","\n","        # Add images\n","        for a in annotations:\n","            # Get the x, y coordinaets of points of the polygons that make up\n","            # the outline of each object instance. These are stores in the\n","            # shape_attributes (see json format above)\n","            # The if condition is needed to support VIA versions 1.x and 2.x.\n","            if type(a['regions']) is dict:\n","                polygons = [r['shape_attributes'] for r in a['regions'].values()]\n","            else:\n","                polygons = [r['shape_attributes'] for r in a['regions']] \n","\n","            # load_mask() needs the image size to convert polygons to masks.\n","            # Unfortunately, VIA doesn't include it in JSON, so we must read\n","            # the image. This is only managable since the dataset is tiny.\n","            image_path = os.path.join(dataset_dir, a['filename'])\n","            image = skimage.io.imread(image_path)\n","            height, width = image.shape[:2]\n","\n","            self.add_image(\n","                \"balloon\",\n","                image_id=a['filename'],  # use file name as a unique image id\n","                path=image_path,\n","                width=width, height=height,\n","                polygons=polygons)\n","\n","    def load_mask(self, image_id):\n","        \"\"\"Generate instance masks for an image.\n","       Returns:\n","        masks: A bool array of shape [height, width, instance count] with\n","            one mask per instance.\n","        class_ids: a 1D array of class IDs of the instance masks.\n","        \"\"\"\n","        # If not a balloon dataset image, delegate to parent class.\n","        image_info = self.image_info[image_id]\n","        if image_info[\"source\"] != \"balloon\":\n","            return super(self.__class__, self).load_mask(image_id)\n","\n","        # Convert polygons to a bitmap mask of shape\n","        # [height, width, instance_count]\n","        info = self.image_info[image_id]\n","        mask = np.zeros([info[\"height\"], info[\"width\"], len(info[\"polygons\"])],\n","                        dtype=np.uint8)\n","        for i, p in enumerate(info[\"polygons\"]):\n","            # Get indexes of pixels inside the polygon and set them to 1\n","            rr, cc = skimage.draw.polygon(p['all_points_y'], p['all_points_x'])\n","            mask[rr, cc, i] = 1\n","\n","        # Return mask, and array of class IDs of each instance. Since we have\n","        # one class ID only, we return an array of 1s\n","        return mask.astype(np.bool), np.ones([mask.shape[-1]], dtype=np.int32)\n","\n","    def image_reference(self, image_id):\n","        \"\"\"Return the path of the image.\"\"\"\n","        info = self.image_info[image_id]\n","        if info[\"source\"] == \"balloon\":\n","            return info[\"path\"]\n","        else:\n","            super(self.__class__, self).image_reference(image_id)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KzQwsdbKEYva"},"source":["def train(model):\n","    \"\"\"Train the model.\"\"\"\n","    # Training dataset.\n","    dataset_train = BalloonDataset()\n","    dataset_train.load_balloon(dataset, \"train\")\n","    dataset_train.prepare()\n","\n","    # Validation dataset\n","    dataset_val = BalloonDataset()\n","    dataset_val.load_balloon(dataset, \"val\")\n","    dataset_val.prepare()\n","\n","    # *** This training schedule is an example. Update to your needs ***\n","    # Since we're using a very small dataset, and starting from\n","    # COCO trained weights, we don't need to train too long. Also,\n","    # no need to train all layers, just the heads should do it.\n","    print(\"Training network heads\")\n","    model.train(dataset_train, dataset_val,\n","                learning_rate=config.LEARNING_RATE,\n","                epochs=30,\n","                layers='heads')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2BpgspRgEcV3","colab":{"base_uri":"https://localhost:8080/"},"outputId":"80750080-ea70-4105-fe87-a48c01499a56"},"source":["dataset = ROOT_DIR + '/datasets/balloon/'\n","\n","# parameter untuk weights\n","# 'coco: load pretrained weights dari coco\n","# 'imagenet': load pretrained weights dari imagenet\n","# 'last': load pretrained weights dari proses training (belum selesai) yang terbaru\n","weights = 'last'\n","logs = DEFAULT_LOGS_DIR\n","\n","print(\"Weights: \", weights)\n","print(\"Dataset: \", dataset)\n","print(\"Logs: \", logs)\n","\n","# Configurations\n","config = BalloonConfig()\n","\n","# Create model\n","model = modellib.MaskRCNN(mode=\"training\", config=config,\n","                              model_dir=logs)\n","\n","\n","# Select weights file to load\n","if weights.lower() == \"coco\":\n","    weights_path = COCO_WEIGHTS_PATH\n","    if not os.path.exists(weights_path):\n","        utils.download_trained_weights(weights_path)\n","elif weights.lower() == \"last\":\n","    # Find last trained weights\n","    weights_path = model.find_last()\n","elif weights.lower() == \"imagenet\":\n","    # Start from ImageNet trained weights\n","    weights_path = model.get_imagenet_weights()\n","else:\n","    weights_path = weights\n","\n","# Load weights\n","print(\"Loading weights \", weights_path)\n","if weights.lower() == \"coco\":\n","    # Exclude the last layers because they require a matching\n","    # number of classes\n","    model.load_weights(weights_path, by_name=True, exclude=[\n","        \"mrcnn_class_logits\", \"mrcnn_bbox_fc\",\n","        \"mrcnn_bbox\", \"mrcnn_mask\"])\n","else:\n","    model.load_weights(weights_path, by_name=True)\n","\n","# Train or evaluate\n","train(model)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Weights:  last\n","Dataset:  /content/drive/MyDrive/Deep Learning/Tugas Kelompok DL/Tugas Implementasi Mask RCNN/Colab/Mask_RCNN-master/datasets/balloon/\n","Logs:  /content/drive/MyDrive/Deep Learning/Tugas Kelompok DL/Tugas Implementasi Mask RCNN/Colab/Mask_RCNN-master/logs\n","Loading weights  /content/drive/MyDrive/Deep Learning/Tugas Kelompok DL/Tugas Implementasi Mask RCNN/Colab/Mask_RCNN-master/logs/balloon20210511T1340/mask_rcnn_balloon_0007.h5\n","Re-starting from epoch 7\n","Training network heads\n","\n","Starting at epoch 7. LR=0.001\n","\n","Checkpoint Path: /content/drive/MyDrive/Deep Learning/Tugas Kelompok DL/Tugas Implementasi Mask RCNN/Colab/Mask_RCNN-master/logs/balloon20210511T1340/mask_rcnn_balloon_{epoch:04d}.h5\n","Selecting layers to train\n","fpn_c5p5               (Conv2D)\n","fpn_c4p4               (Conv2D)\n","fpn_c3p3               (Conv2D)\n","fpn_c2p2               (Conv2D)\n","fpn_p5                 (Conv2D)\n","fpn_p2                 (Conv2D)\n","fpn_p3                 (Conv2D)\n","fpn_p4                 (Conv2D)\n","In model:  rpn_model\n","    rpn_conv_shared        (Conv2D)\n","    rpn_class_raw          (Conv2D)\n","    rpn_bbox_pred          (Conv2D)\n","mrcnn_mask_conv1       (TimeDistributed)\n","mrcnn_mask_bn1         (TimeDistributed)\n","mrcnn_mask_conv2       (TimeDistributed)\n","mrcnn_mask_bn2         (TimeDistributed)\n","mrcnn_class_conv1      (TimeDistributed)\n","mrcnn_class_bn1        (TimeDistributed)\n","mrcnn_mask_conv3       (TimeDistributed)\n","mrcnn_mask_bn3         (TimeDistributed)\n","mrcnn_class_conv2      (TimeDistributed)\n","mrcnn_class_bn2        (TimeDistributed)\n","mrcnn_mask_conv4       (TimeDistributed)\n","mrcnn_mask_bn4         (TimeDistributed)\n","mrcnn_bbox_fc          (TimeDistributed)\n","mrcnn_mask_deconv      (TimeDistributed)\n","mrcnn_class_logits     (TimeDistributed)\n","mrcnn_mask             (TimeDistributed)\n","WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... can't pickle _thread.RLock objects\n","Epoch 8/30\n","100/100 [==============================] - 363s 4s/step - loss: 0.1923 - val_loss: 0.6248\n","Epoch 9/30\n","100/100 [==============================] - 296s 3s/step - loss: 0.2159 - val_loss: 1.7448\n","Epoch 10/30\n","100/100 [==============================] - 295s 3s/step - loss: 0.2158 - val_loss: 2.0255\n","Epoch 11/30\n","100/100 [==============================] - 291s 3s/step - loss: 0.2134 - val_loss: 0.5595\n","Epoch 12/30\n","100/100 [==============================] - 291s 3s/step - loss: 0.2094 - val_loss: 0.1206\n","Epoch 13/30\n","100/100 [==============================] - 295s 3s/step - loss: 0.1749 - val_loss: 1.8904\n","Epoch 14/30\n"," 99/100 [============================>.] - ETA: 2s - loss: 0.1880"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"MwuxSNoApjhH"},"source":[""],"execution_count":null,"outputs":[]}]}